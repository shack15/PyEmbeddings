{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 1: Unstructured default tokenization\n",
    "\n",
    "# Install the packages and libmagic for automatic file type detection\n",
    "pip install \"unstructured[all-docs]\"\n",
    "pip install unstructured-client\n",
    "brew install libmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_client import UnstructuredClient\n",
    "s = UnstructuredClient(api_key_auth=\"UXNnvEtTT7FyVI1qY1R85616zcN8eO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./gpt-4.pdf\"\n",
    "file = open(filename, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_client.models import shared\n",
    "\n",
    "req = shared.PartitionParameters(\n",
    "    # Note that this currently only supports a single file\n",
    "    files=shared.Files(\n",
    "        content=file.read(),\n",
    "        file_name=filename,\n",
    "    ),\n",
    "    # Other partition params\n",
    "    strategy=\"fast\", # fast, hi_res, auto. For details see https://unstructured-io.github.io/unstructured/best_practices/strategies.html\n",
    ")\n",
    "print(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Unstructured partitioning\n",
    "\n",
    "from unstructured_client.models.errors import SDKError\n",
    "\n",
    "try:\n",
    "    res = s.general.partition(req)\n",
    "    for i in range(10):\n",
    "        print(res.elements[i])\n",
    "except SDKError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 2: Proposition-based tokenization\n",
    "\n",
    "# Install package to read PDF\n",
    "pip install PyPDF2\n",
    "pip install transformers\n",
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposition-based retrieval\n",
    "\n",
    "# Read all text from PDF\n",
    "import PyPDF2\n",
    "reader = PyPDF2.PdfReader('gpt-4.pdf')\n",
    "content = \"\"\n",
    "for i in range(len(reader.pages)):\n",
    "    content += reader.pages[i].extract_text()\n",
    "\n",
    "# Use a small portion of text for testing\n",
    "content = content[:2000]\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_name = \"chentong00/propositionizer-wiki-flan-t5-large\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "input_ids = tokenizer(content, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids.to(device), max_new_tokens=512).cpu()\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "try:\n",
    "    prop_list = json.loads(output_text)\n",
    "except:\n",
    "    prop_list = []\n",
    "    print(\"[ERROR] Failed to parse output text as JSON.\")\n",
    "print(json.dumps(prop_list, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 2: Parent document retriever\n",
    "\n",
    "# Install langchain for proof of concept\n",
    "pip install langchain\n",
    "pip install tiktoken\n",
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "GPT-4\n",
      "1844\n",
      "GPT-4 (launch)2.9PotentialforRiskyEmergentBehaviorsNovelcapabilitiesoftenemergeinmorepowerfulmodels.[60,61]Somethatareparticularlyconcerningaretheabilitytocreateandactonlong-termplans,[62]toaccruepowerandresources(“power-seeking”),[63]andtoexhibitbehaviorthatisincreasingly“agentic.”[64]Agenticinthiscontextdoesnotintendtohumanizelanguagemodelsorrefertosentiencebutratherreferstosystemscharacterizedbyabilityto,e.g.,accomplishgoalswhichmaynothavebeenconcretelyspeciﬁedand54\n",
      "\n",
      "Prompt\n",
      "\n",
      "1. Insecure password hashing: The code uses MD5 for hashing passwords, which is considered insecure due to its vulnerability to collision attacks and its speed, allowing attackers to perform brute force attacks more easily. A more secure alternative would be to use bcrypt or Argon2.2. SQL Injection: The ``fetch'' function constructs an SQL query using string concatenation with unsanitized user input, which could allow an attacker to inject malicious SQL code. To mitigate this risk, you should use parameterized queries or prepared statements.3. JWT Secret Hardcoded: The JWT secret key is hardcoded in the ``loginHandler'' function, which is not a good practice. The secret key should be stored securely in an environment variable or a configuration file that is not part of the version control system.4. Error handling: The code does not check for errors when calling the ``token.SignedString(key)'' function, which could lead to unexpected behavior. It's important to handle errors properly to avoid potential security issues.5. Lack of HTTPS: The server is listening on HTTP, which means that the communication between the client and the server is not encrypted. To ensure secure communication, use HTTPS instead.\n",
      "\n",
      "You are an expert penetration tester.List any vulnerabilities in the code below. If there are none, respond with \"none\".[Code in Appendix]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "loader = UnstructuredFileLoader(\"gpt-4.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# This text splitter is used to create the parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "# This text splitter is used to create the child documents\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings(disallowed_special=()), persist_directory=\"./chroma_db\"\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)\n",
    "\n",
    "print(len(list(store.yield_keys())))\n",
    "\n",
    "sub_docs = vectorstore.similarity_search(\"GPT-4\")\n",
    "print(sub_docs[0].page_content)\n",
    "\n",
    "retrieved_docs = retriever.get_relevant_documents(\"GPT-4\")\n",
    "print(len(retrieved_docs[0].page_content))\n",
    "print(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
